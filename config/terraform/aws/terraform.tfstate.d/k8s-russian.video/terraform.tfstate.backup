{
  "version": 4,
  "terraform_version": "0.12.6",
  "serial": 64,
  "lineage": "438db69e-cebd-5b92-b4b1-fd0a3ed6ecc6",
  "outputs": {
    "master_dns": {
      "value": "ec2-52-90-121-0.compute-1.amazonaws.com",
      "type": "string"
    }
  },
  "resources": [
    {
      "mode": "data",
      "type": "aws_ami",
      "name": "latest_ami",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "architecture": "x86_64",
            "block_device_mappings": [
              {
                "device_name": "/dev/sda1",
                "ebs": {
                  "delete_on_termination": "true",
                  "encrypted": "false",
                  "iops": "0",
                  "snapshot_id": "snap-05c47a3b3d7bca907",
                  "volume_size": "8",
                  "volume_type": "gp2"
                },
                "no_device": "",
                "virtual_name": ""
              },
              {
                "device_name": "/dev/sdb",
                "ebs": {},
                "no_device": "",
                "virtual_name": "ephemeral0"
              },
              {
                "device_name": "/dev/sdc",
                "ebs": {},
                "no_device": "",
                "virtual_name": "ephemeral1"
              }
            ],
            "creation_date": "2019-07-25T19:49:07.000Z",
            "description": "Canonical, Ubuntu, 18.04 LTS, amd64 bionic image build on 2019-07-22",
            "executable_users": null,
            "filter": [
              {
                "name": "name",
                "values": [
                  "ubuntu/images/hvm-ssd/ubuntu-bionic-18.04-amd64-server-*"
                ]
              },
              {
                "name": "root-device-type",
                "values": [
                  "ebs"
                ]
              },
              {
                "name": "virtualization-type",
                "values": [
                  "hvm"
                ]
              }
            ],
            "hypervisor": "xen",
            "id": "ami-07d0cf3af28718ef8",
            "image_id": "ami-07d0cf3af28718ef8",
            "image_location": "099720109477/ubuntu/images/hvm-ssd/ubuntu-bionic-18.04-amd64-server-20190722.1",
            "image_owner_alias": null,
            "image_type": "machine",
            "kernel_id": null,
            "most_recent": true,
            "name": "ubuntu/images/hvm-ssd/ubuntu-bionic-18.04-amd64-server-20190722.1",
            "name_regex": null,
            "owner_id": "099720109477",
            "owners": [
              "099720109477"
            ],
            "platform": null,
            "product_codes": [],
            "public": true,
            "ramdisk_id": null,
            "root_device_name": "/dev/sda1",
            "root_device_type": "ebs",
            "root_snapshot_id": "snap-05c47a3b3d7bca907",
            "sriov_net_support": "simple",
            "state": "available",
            "state_reason": {
              "code": "UNSET",
              "message": "UNSET"
            },
            "tags": {},
            "virtualization_type": "hvm"
          }
        }
      ]
    },
    {
      "mode": "data",
      "type": "template_file",
      "name": "cert-manager-issuer-manifest",
      "provider": "provider.template",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "filename": null,
            "id": "ef3a29e0b8e7805c7afdca04f6a02b50043336794e3ec54f1ab03746e2e57b6f",
            "rendered": "apiVersion: certmanager.k8s.io/v1beta1\nkind: Issuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    # The ACME server URL\n    server: https://acme-v02.api.letsencrypt.org/directory\n    # Email address used for ACME registration\n    email: bloodjazman@gmail.com\n    # Name of a secret used to store the ACME account private key\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    # Enable the HTTP-01 challenge provider\n    http01: {}\n",
            "template": "apiVersion: certmanager.k8s.io/v1beta1\nkind: Issuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    # The ACME server URL\n    server: https://acme-v02.api.letsencrypt.org/directory\n    # Email address used for ACME registration\n    email: ${cert_manager_email}\n    # Name of a secret used to store the ACME account private key\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    # Enable the HTTP-01 challenge provider\n    http01: {}\n",
            "vars": {
              "cert_manager_email": "bloodjazman@gmail.com"
            }
          }
        }
      ]
    },
    {
      "mode": "data",
      "type": "template_file",
      "name": "cluster-autoscaler-manifest",
      "provider": "provider.template",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "filename": null,
            "id": "4e2079d8bbccfa4cc26c6c19ab65a9d373709ba41a21b00bd864f52699dceae7",
            "rendered": "---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    k8s-addon: cluster-autoscaler.addons.k8s.io\n    k8s-app: cluster-autoscaler\n  name: cluster-autoscaler\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: cluster-autoscaler\n  labels:\n    k8s-addon: cluster-autoscaler.addons.k8s.io\n    k8s-app: cluster-autoscaler\nrules:\n- apiGroups: [\"\"]\n  resources: [\"events\",\"endpoints\"]\n  verbs: [\"create\", \"patch\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/eviction\"]\n  verbs: [\"create\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/status\"]\n  verbs: [\"update\"]\n- apiGroups: [\"\"]\n  resources: [\"endpoints\"]\n  resourceNames: [\"cluster-autoscaler\"]\n  verbs: [\"get\",\"update\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"watch\",\"list\",\"get\",\"update\"]\n- apiGroups: [\"\"]\n  resources: [\"pods\",\"services\",\"replicationcontrollers\",\"persistentvolumeclaims\",\"persistentvolumes\"]\n  verbs: [\"watch\",\"list\",\"get\"]\n- apiGroups: [\"extensions\"]\n  resources: [\"replicasets\",\"daemonsets\"]\n  verbs: [\"watch\",\"list\",\"get\"]\n- apiGroups: [\"policy\"]\n  resources: [\"poddisruptionbudgets\"]\n  verbs: [\"watch\",\"list\"]\n- apiGroups: [\"apps\"]\n  resources: [\"statefulsets\",\"replicasets\"]\n  verbs: [\"watch\",\"list\",\"get\"]\n- apiGroups: [\"storage.k8s.io\"]\n  resources: [\"storageclasses\"]\n  verbs: [\"watch\",\"list\",\"get\"]\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: cluster-autoscaler\n  namespace: kube-system\n  labels:\n    k8s-addon: cluster-autoscaler.addons.k8s.io\n    k8s-app: cluster-autoscaler\nrules:\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  verbs: [\"create\"]\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  resourceNames: [\"cluster-autoscaler-status\"]\n  verbs: [\"delete\",\"get\",\"update\"]\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: cluster-autoscaler\n  labels:\n    k8s-addon: cluster-autoscaler.addons.k8s.io\n    k8s-app: cluster-autoscaler\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-autoscaler\nsubjects:\n  - kind: ServiceAccount\n    name: cluster-autoscaler\n    namespace: kube-system\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: cluster-autoscaler\n  namespace: kube-system\n  labels:\n    k8s-addon: cluster-autoscaler.addons.k8s.io\n    k8s-app: cluster-autoscaler\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: cluster-autoscaler\nsubjects:\n  - kind: ServiceAccount\n    name: cluster-autoscaler\n    namespace: kube-system\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cluster-autoscaler\n  namespace: kube-system\n  labels:\n    app: cluster-autoscaler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cluster-autoscaler\n  template:\n    metadata:\n      labels:\n        app: cluster-autoscaler\n    spec:\n      serviceAccountName: cluster-autoscaler\n      nodeSelector:\n        node-role.kubernetes.io/master: \"\"\n      containers:\n        - image: gcr.io/google-containers/cluster-autoscaler:latest\n          name: cluster-autoscaler\n          resources:\n            limits:\n              cpu: 100m\n              memory: 300Mi\n            requests:\n              cpu: 100m\n              memory: 300Mi\n          command:\n            - ./cluster-autoscaler\n            - --v=4\n            - --stderrthreshold=info\n            - --cloud-provider=aws\n            - --skip-nodes-with-local-storage=false\n            - --expander=least-waste\n            - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,kubernetes.io/cluster/k8s\n          env:\n            - name: AWS_REGION\n              value: us-east-1\n          volumeMounts:\n            - name: ssl-certs\n              mountPath: /etc/kubernetes/pki/ca.crt\n              readOnly: true\n      volumes:\n        - name: ssl-certs\n          hostPath:\n            path: \"/etc/kubernetes/pki/ca.crt\"\n",
            "template": "---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    k8s-addon: cluster-autoscaler.addons.k8s.io\n    k8s-app: cluster-autoscaler\n  name: cluster-autoscaler\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: cluster-autoscaler\n  labels:\n    k8s-addon: cluster-autoscaler.addons.k8s.io\n    k8s-app: cluster-autoscaler\nrules:\n- apiGroups: [\"\"]\n  resources: [\"events\",\"endpoints\"]\n  verbs: [\"create\", \"patch\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/eviction\"]\n  verbs: [\"create\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/status\"]\n  verbs: [\"update\"]\n- apiGroups: [\"\"]\n  resources: [\"endpoints\"]\n  resourceNames: [\"cluster-autoscaler\"]\n  verbs: [\"get\",\"update\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"watch\",\"list\",\"get\",\"update\"]\n- apiGroups: [\"\"]\n  resources: [\"pods\",\"services\",\"replicationcontrollers\",\"persistentvolumeclaims\",\"persistentvolumes\"]\n  verbs: [\"watch\",\"list\",\"get\"]\n- apiGroups: [\"extensions\"]\n  resources: [\"replicasets\",\"daemonsets\"]\n  verbs: [\"watch\",\"list\",\"get\"]\n- apiGroups: [\"policy\"]\n  resources: [\"poddisruptionbudgets\"]\n  verbs: [\"watch\",\"list\"]\n- apiGroups: [\"apps\"]\n  resources: [\"statefulsets\",\"replicasets\"]\n  verbs: [\"watch\",\"list\",\"get\"]\n- apiGroups: [\"storage.k8s.io\"]\n  resources: [\"storageclasses\"]\n  verbs: [\"watch\",\"list\",\"get\"]\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: cluster-autoscaler\n  namespace: kube-system\n  labels:\n    k8s-addon: cluster-autoscaler.addons.k8s.io\n    k8s-app: cluster-autoscaler\nrules:\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  verbs: [\"create\"]\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  resourceNames: [\"cluster-autoscaler-status\"]\n  verbs: [\"delete\",\"get\",\"update\"]\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: cluster-autoscaler\n  labels:\n    k8s-addon: cluster-autoscaler.addons.k8s.io\n    k8s-app: cluster-autoscaler\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-autoscaler\nsubjects:\n  - kind: ServiceAccount\n    name: cluster-autoscaler\n    namespace: kube-system\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: cluster-autoscaler\n  namespace: kube-system\n  labels:\n    k8s-addon: cluster-autoscaler.addons.k8s.io\n    k8s-app: cluster-autoscaler\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: cluster-autoscaler\nsubjects:\n  - kind: ServiceAccount\n    name: cluster-autoscaler\n    namespace: kube-system\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cluster-autoscaler\n  namespace: kube-system\n  labels:\n    app: cluster-autoscaler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cluster-autoscaler\n  template:\n    metadata:\n      labels:\n        app: cluster-autoscaler\n    spec:\n      serviceAccountName: cluster-autoscaler\n      nodeSelector:\n        node-role.kubernetes.io/master: \"\"\n      containers:\n        - image: gcr.io/google-containers/cluster-autoscaler:latest\n          name: cluster-autoscaler\n          resources:\n            limits:\n              cpu: 100m\n              memory: 300Mi\n            requests:\n              cpu: 100m\n              memory: 300Mi\n          command:\n            - ./cluster-autoscaler\n            - --v=4\n            - --stderrthreshold=info\n            - --cloud-provider=aws\n            - --skip-nodes-with-local-storage=false\n            - --expander=least-waste\n            - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,kubernetes.io/cluster/${cluster_name}\n          env:\n            - name: AWS_REGION\n              value: ${cluster_region}\n          volumeMounts:\n            - name: ssl-certs\n              mountPath: /etc/kubernetes/pki/ca.crt\n              readOnly: true\n      volumes:\n        - name: ssl-certs\n          hostPath:\n            path: \"/etc/kubernetes/pki/ca.crt\"\n",
            "vars": {
              "cluster_name": "k8s",
              "cluster_region": "us-east-1"
            }
          }
        }
      ]
    },
    {
      "mode": "data",
      "type": "template_file",
      "name": "master-userdata",
      "provider": "provider.template",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "filename": null,
            "id": "2b875dc5989712160df00fbc13e143c38d2e1f542929bdfc634c48d50a742442",
            "rendered": "#!/usr/bin/env bash\nset -exuv -o pipefail\n\n# kube-router network settings\napt-get install -y ipvsadm\nmodprobe ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh\nmodprobe overlay br_netfilter\n# Setup required sysctl params, these persist across reboots.\ncat \u003e /etc/sysctl.d/99-kubernetes.conf \u003c\u003cEOF\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\nsysctl --system\nservice procps start\n\n# Disable pointless daemons\nsystemctl stop snapd snapd.socket lxcfs snap.amazon-ssm-agent.amazon-ssm-agent\nsystemctl disable snapd snapd.socket lxcfs snap.amazon-ssm-agent.amazon-ssm-agent\n\n# Disable swap to make K8S happy\nswapoff -a\nsed -i '/swap/d' /etc/fstab\n\n\n# Install K8S, kubeadm and containerd\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\necho \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" \u003e /etc/apt/sources.list.d/kubernetes.list\nexport DEBIAN_FRONTEND=noninteractive\napt-get update\napt-get install -y kubelet=1.15.0-00 kubeadm=1.15.0-00 kubectl=1.15.0-00 awscli jq containerd.io\napt-mark hold kubelet kubeadm kubectl containerd.io\n\n# Install etcdctl for the version of etcd we're running\nETCD_VERSION=$(kubeadm config images list | grep etcd | cut -d':' -f2)\nwget \"https://github.com/coreos/etcd/releases/download/v${ETCD_VERSION}/etcd-v${ETCD_VERSION}-linux-amd64.tar.gz\"\ntar xvf \"etcd-v${ETCD_VERSION}-linux-amd64.tar.gz\"\nmv \"etcd-v${ETCD_VERSION}-linux-amd64/etcdctl\" /usr/local/bin/\nrm -rf etcd*\n\n# Point Docker at big ephemeral drive and turn on log rotation\nsystemctl enable containerd\nsystemctl restart containerd\n\n# Work around the fact spot requests can't tag their instances\nREGION=$(ec2metadata --availability-zone | rev | cut -c 2- | rev)\nINSTANCE_ID=$(ec2metadata --instance-id)\naws --region $REGION ec2 create-tags --resources $INSTANCE_ID --tags \"Key=Name,Value=k8s-master\" \"Key=Environment,Value=k8s\" \"Key=kubernetes.io/cluster/k8s,Value=owned\"\n\n# Point kubelet at big ephemeral drive\nmkdir /mnt/kubelet\necho 'KUBELET_EXTRA_ARGS=\"--root-dir=/mnt/kubelet --cloud-provider=aws\"' \u003e /etc/default/kubelet\n\ncat \u003einit-config.yaml \u003c\u003cEOF\napiVersion: kubeadm.k8s.io/v1beta2\nkind: InitConfiguration\nbootstrapTokens:\n- groups:\n  - system:bootstrappers:kubeadm:default-node-token\n  token: \"xrr5lk.v8fihtfb83fqlsir\"\n  ttl: \"0\"\nnodeRegistration:\n  name: \"$(hostname -f)\"\n  taints: []\n---\napiVersion: kubeadm.k8s.io/v1beta2\nkind: ClusterConfiguration\napiServerExtraArgs:\n  cloud-provider: aws\ncontrollerManagerExtraArgs:\n  cloud-provider: aws\nnetworking:\n  podSubnet: 10.244.0.0/16\nEOF\n\n# Check if there is an etcd backup on the s3 bucket and restore from it if there is\nif [[ $(aws s3 ls s3://k8s20190804144427827600000001/etcd-backups/ | wc -l) -ne 0 ]]; then\n  echo \"Found existing etcd backup. Restoring from it instead of starting from fresh.\"\n  latest_backup=$(aws s3api list-objects --bucket k8s20190804144427827600000001 --prefix etcd-backups --query 'reverse(sort_by(Contents,\u0026LastModified))[0]' | jq -rc .Key)\n\n  echo \"Downloading latest etcd backup\"\n  aws s3 cp s3://k8s20190804144427827600000001/$latest_backup etcd-snapshot.db\n  old_instance_id=$(echo $latest_backup | cut -d'/' -f2)\n\n  echo \"Downloading kubernetes ca cert and key backup\"\n  aws s3 cp s3://k8s20190804144427827600000001/pki/$old_instance_id/ca.crt /etc/kubernetes/pki/ca.crt\n  chmod 644 /etc/kubernetes/pki/ca.crt\n  aws s3 cp s3://k8s20190804144427827600000001/pki/$old_instance_id/ca.key /etc/kubernetes/pki/ca.key\n  chmod 600 /etc/kubernetes/pki/ca.key\n\n  echo \"Restoring downloaded etcd backup\"\n  ETCDCTL_API=3 /usr/local/bin/etcdctl snapshot restore etcd-snapshot.db\n  mkdir -p /var/lib/etcd\n  mv default.etcd/member /var/lib/etcd/\n\n  echo \"Running kubeadm init\"\n  kubeadm init --ignore-preflight-errors=\"DirAvailable--var-lib-etcd,NumCPU\" --config=init-config.yaml\nelse\n  echo \"Running kubeadm init\"\n  kubeadm init --config=init-config.yaml --ignore-preflight-errors=NumCPU\n  touch /tmp/fresh-cluster\nfi\n\n\n# Set up kubectl for the ubuntu user\nmkdir -p /home/ubuntu/.kube \u0026\u0026 cp -i /etc/kubernetes/admin.conf /home/ubuntu/.kube/config \u0026\u0026 chown -R ubuntu. /home/ubuntu/.kube\necho 'source \u003c(kubectl completion bash)' \u003e\u003e /home/ubuntu/.bashrc\n\n# Install helm\nwget https://get.helm.sh/helm-v1.14.3-linux-amd64.tar.gz\ntar xvf helm-v1.14.3-linux-amd64.tar.gz\nmv linux-amd64/helm /usr/local/bin/\nrm -rf linux-amd64 helm-*\n\nif [[ -f /tmp/fresh-cluster ]]; then\n  # uninstall kube-proxy\n  while [[ \"0\" == $(crictl ps | grep kube-proxy | grep -i running | wc -l ) ]]; do\n    echo \"Kube-proxy is not yet ready\"\n    sleep 3\n  done\n  K8S_VERSION=$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable-1.txt | tr -d '\\n')\n  ctr run --rm --privileged --mount type=bind,src=/lib/modules,dst=/lib/modules --net-host k8s.gcr.io/kube-proxy:${K8S_VERSION} kube-proxy --cleanup || true\n  su -c 'kubectl -n kube-system delete ds kube-proxy' ubuntu\n\n  # Install kube-router networking.\n  su -c 'kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter-all-features.yaml' ubuntu\n\n  # Set up helm\n  su -c 'kubectl create serviceaccount tiller --namespace=kube-system' ubuntu\n  su -c 'kubectl create clusterrolebinding tiller-admin --serviceaccount=kube-system:tiller --clusterrole=cluster-admin' ubuntu\n  su -c 'helm init --service-account=tiller' ubuntu\n\n  # Install cert-manager\n  if [[ \"1\" == \"1\" ]]; then\n    sleep 60 # Give Tiller a minute to start up\n    su -c 'helm install --name cert-manager --namespace cert-manager --version 0.5.2 stable/cert-manager --set createCustomResource=false \u0026\u0026 helm upgrade --install --namespace cert-manager --version 0.5.2 cert-manager stable/cert-manager --set createCustomResource=true' ubuntu\n  fi\n\n  # Install all the YAML we've put on S3\n  mkdir /tmp/manifests\n  aws s3 sync s3://k8s20190804144427827600000001/manifests/ /tmp/manifests\n  su -c 'kubectl apply -f /tmp/manifests/' ubuntu\nfi\n\n# Set up backups if they have been enabled\nif [[ \"1\" == \"1\" ]]; then\n  # One time backup of kubernetes directory\n  aws s3 cp /etc/kubernetes/pki/ca.crt s3://k8s20190804144427827600000001/pki/$INSTANCE_ID/\n  aws s3 cp /etc/kubernetes/pki/ca.key s3://k8s20190804144427827600000001/pki/$INSTANCE_ID/\n\n  # Back up etcd to s3 every 15 minutes. The lifecycle policy in terraform will keep 7 days to save us doing that logic here.\n  cat \u003c\u003c-EOF \u003e /usr/local/bin/backup-etcd.sh\n\t#!/bin/bash\n\tETCDCTL_API=3 /usr/local/bin/etcdctl --cacert='/etc/kubernetes/pki/etcd/ca.crt' --cert='/etc/kubernetes/pki/etcd/peer.crt' --key='/etc/kubernetes/pki/etcd/peer.key' snapshot save etcd-snapshot.db\n\taws s3 cp etcd-snapshot.db s3://k8s20190804144427827600000001/etcd-backups/$INSTANCE_ID/etcd-snapshot-\\$(date -Iseconds).db\n\tEOF\n\n  echo \"*/15 * * * * root bash /usr/local/bin/backup-etcd.sh\" \u003e /etc/cron.d/backup-etcd\n\n  # Poll the spot instance termination URL and backup immediately if it returns a 200 response.\n  cat \u003c\u003c-'EOF' \u003e /usr/local/bin/check-termination.sh\n\t#!/bin/bash\n\t# Mostly borrowed from https://github.com/kube-aws/kube-spot-termination-notice-handler/blob/master/entrypoint.sh\n\t\n\tPOLL_INTERVAL=10\n\tNOTICE_URL=\"http://169.254.169.254/latest/meta-data/spot/termination-time\"\n\t\n\techo \"Polling ${NOTICE_URL} every ${POLL_INTERVAL} second(s)\"\n\t\n\t# To whom it may concern: http://superuser.com/questions/590099/can-i-make-curl-fail-with-an-exitcode-different-than-0-if-the-http-status-code-i\n\twhile http_status=$(curl -o /dev/null -w '%{http_code}' -sL ${NOTICE_URL}); [ ${http_status} -ne 200 ]; do\n\t  echo \"Polled termination notice URL. HTTP Status was ${http_status}.\"\n\t  sleep ${POLL_INTERVAL}\n\tdone\n\t\n\techo \"Polled termination notice URL. HTTP Status was ${http_status}. Triggering backup.\"\n\t/bin/bash /usr/local/bin/backup-etcd.sh\n\tsleep 300 # Sleep for 5 minutes, by which time the machine will have terminated.\n\tEOF\n\n  cat \u003c\u003c-'EOF' \u003e /etc/systemd/system/check-termination.service\n\t[Unit]\n\tDescription=Spot Termination Checker\n\tAfter=network.target\n\t[Service]\n\tType=simple\n\tRestart=always\n\tRestartSec=10\n\tUser=root\n\tExecStart=/bin/bash /usr/local/bin/check-termination.sh\n\t\n\t[Install]\n\tWantedBy=multi-user.target\n\tEOF\n\n  systemctl start check-termination\n  systemctl enable check-termination\nfi\n\nsu -c \"kubectl get nodes -o wide | grep ${HOSTNAME}\" ubuntu\n",
            "template": "#!/usr/bin/env bash\nset -exuv -o pipefail\n\n# kube-router network settings\napt-get install -y ipvsadm\nmodprobe ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh\nmodprobe overlay br_netfilter\n# Setup required sysctl params, these persist across reboots.\ncat \u003e /etc/sysctl.d/99-kubernetes.conf \u003c\u003cEOF\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\nsysctl --system\nservice procps start\n\n# Disable pointless daemons\nsystemctl stop snapd snapd.socket lxcfs snap.amazon-ssm-agent.amazon-ssm-agent\nsystemctl disable snapd snapd.socket lxcfs snap.amazon-ssm-agent.amazon-ssm-agent\n\n# Disable swap to make K8S happy\nswapoff -a\nsed -i '/swap/d' /etc/fstab\n\n\n# Install K8S, kubeadm and containerd\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\necho \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" \u003e /etc/apt/sources.list.d/kubernetes.list\nexport DEBIAN_FRONTEND=noninteractive\napt-get update\napt-get install -y kubelet=${k8sversion}-00 kubeadm=${k8sversion}-00 kubectl=${k8sversion}-00 awscli jq containerd.io\napt-mark hold kubelet kubeadm kubectl containerd.io\n\n# Install etcdctl for the version of etcd we're running\nETCD_VERSION=$(kubeadm config images list | grep etcd | cut -d':' -f2)\nwget \"https://github.com/coreos/etcd/releases/download/v$${ETCD_VERSION}/etcd-v$${ETCD_VERSION}-linux-amd64.tar.gz\"\ntar xvf \"etcd-v$${ETCD_VERSION}-linux-amd64.tar.gz\"\nmv \"etcd-v$${ETCD_VERSION}-linux-amd64/etcdctl\" /usr/local/bin/\nrm -rf etcd*\n\n# Point Docker at big ephemeral drive and turn on log rotation\nsystemctl enable containerd\nsystemctl restart containerd\n\n# Work around the fact spot requests can't tag their instances\nREGION=$(ec2metadata --availability-zone | rev | cut -c 2- | rev)\nINSTANCE_ID=$(ec2metadata --instance-id)\naws --region $REGION ec2 create-tags --resources $INSTANCE_ID --tags \"Key=Name,Value=${clustername}-master\" \"Key=Environment,Value=${clustername}\" \"Key=kubernetes.io/cluster/${clustername},Value=owned\"\n\n# Point kubelet at big ephemeral drive\nmkdir /mnt/kubelet\necho 'KUBELET_EXTRA_ARGS=\"--root-dir=/mnt/kubelet --cloud-provider=aws\"' \u003e /etc/default/kubelet\n\ncat \u003einit-config.yaml \u003c\u003cEOF\napiVersion: kubeadm.k8s.io/v1beta2\nkind: InitConfiguration\nbootstrapTokens:\n- groups:\n  - system:bootstrappers:kubeadm:default-node-token\n  token: \"${k8stoken}\"\n  ttl: \"0\"\nnodeRegistration:\n  name: \"$(hostname -f)\"\n  taints: []\n---\napiVersion: kubeadm.k8s.io/v1beta2\nkind: ClusterConfiguration\napiServerExtraArgs:\n  cloud-provider: aws\ncontrollerManagerExtraArgs:\n  cloud-provider: aws\nnetworking:\n  podSubnet: 10.244.0.0/16\nEOF\n\n# Check if there is an etcd backup on the s3 bucket and restore from it if there is\nif [[ $(aws s3 ls s3://${s3bucket}/etcd-backups/ | wc -l) -ne 0 ]]; then\n  echo \"Found existing etcd backup. Restoring from it instead of starting from fresh.\"\n  latest_backup=$(aws s3api list-objects --bucket ${s3bucket} --prefix etcd-backups --query 'reverse(sort_by(Contents,\u0026LastModified))[0]' | jq -rc .Key)\n\n  echo \"Downloading latest etcd backup\"\n  aws s3 cp s3://${s3bucket}/$latest_backup etcd-snapshot.db\n  old_instance_id=$(echo $latest_backup | cut -d'/' -f2)\n\n  echo \"Downloading kubernetes ca cert and key backup\"\n  aws s3 cp s3://${s3bucket}/pki/$old_instance_id/ca.crt /etc/kubernetes/pki/ca.crt\n  chmod 644 /etc/kubernetes/pki/ca.crt\n  aws s3 cp s3://${s3bucket}/pki/$old_instance_id/ca.key /etc/kubernetes/pki/ca.key\n  chmod 600 /etc/kubernetes/pki/ca.key\n\n  echo \"Restoring downloaded etcd backup\"\n  ETCDCTL_API=3 /usr/local/bin/etcdctl snapshot restore etcd-snapshot.db\n  mkdir -p /var/lib/etcd\n  mv default.etcd/member /var/lib/etcd/\n\n  echo \"Running kubeadm init\"\n  kubeadm init --ignore-preflight-errors=\"DirAvailable--var-lib-etcd,NumCPU\" --config=init-config.yaml\nelse\n  echo \"Running kubeadm init\"\n  kubeadm init --config=init-config.yaml --ignore-preflight-errors=NumCPU\n  touch /tmp/fresh-cluster\nfi\n\n\n# Set up kubectl for the ubuntu user\nmkdir -p /home/ubuntu/.kube \u0026\u0026 cp -i /etc/kubernetes/admin.conf /home/ubuntu/.kube/config \u0026\u0026 chown -R ubuntu. /home/ubuntu/.kube\necho 'source \u003c(kubectl completion bash)' \u003e\u003e /home/ubuntu/.bashrc\n\n# Install helm\nwget https://get.helm.sh/helm-v${helmversion}-linux-amd64.tar.gz\ntar xvf helm-v${helmversion}-linux-amd64.tar.gz\nmv linux-amd64/helm /usr/local/bin/\nrm -rf linux-amd64 helm-*\n\nif [[ -f /tmp/fresh-cluster ]]; then\n  # uninstall kube-proxy\n  while [[ \"0\" == $(crictl ps | grep kube-proxy | grep -i running | wc -l ) ]]; do\n    echo \"Kube-proxy is not yet ready\"\n    sleep 3\n  done\n  K8S_VERSION=$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable-1.txt | tr -d '\\n')\n  ctr run --rm --privileged --mount type=bind,src=/lib/modules,dst=/lib/modules --net-host k8s.gcr.io/kube-proxy:$${K8S_VERSION} kube-proxy --cleanup || true\n  su -c 'kubectl -n kube-system delete ds kube-proxy' ubuntu\n\n  # Install kube-router networking.\n  su -c 'kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter-all-features.yaml' ubuntu\n\n  # Set up helm\n  su -c 'kubectl create serviceaccount tiller --namespace=kube-system' ubuntu\n  su -c 'kubectl create clusterrolebinding tiller-admin --serviceaccount=kube-system:tiller --clusterrole=cluster-admin' ubuntu\n  su -c 'helm init --service-account=tiller' ubuntu\n\n  # Install cert-manager\n  if [[ \"${certmanagerenabled}\" == \"1\" ]]; then\n    sleep 60 # Give Tiller a minute to start up\n    su -c 'helm install --name cert-manager --namespace cert-manager --version 0.5.2 stable/cert-manager --set createCustomResource=false \u0026\u0026 helm upgrade --install --namespace cert-manager --version 0.5.2 cert-manager stable/cert-manager --set createCustomResource=true' ubuntu\n  fi\n\n  # Install all the YAML we've put on S3\n  mkdir /tmp/manifests\n  aws s3 sync s3://${s3bucket}/manifests/ /tmp/manifests\n  su -c 'kubectl apply -f /tmp/manifests/' ubuntu\nfi\n\n# Set up backups if they have been enabled\nif [[ \"${backupenabled}\" == \"1\" ]]; then\n  # One time backup of kubernetes directory\n  aws s3 cp /etc/kubernetes/pki/ca.crt s3://${s3bucket}/pki/$INSTANCE_ID/\n  aws s3 cp /etc/kubernetes/pki/ca.key s3://${s3bucket}/pki/$INSTANCE_ID/\n\n  # Back up etcd to s3 every 15 minutes. The lifecycle policy in terraform will keep 7 days to save us doing that logic here.\n  cat \u003c\u003c-EOF \u003e /usr/local/bin/backup-etcd.sh\n\t#!/bin/bash\n\tETCDCTL_API=3 /usr/local/bin/etcdctl --cacert='/etc/kubernetes/pki/etcd/ca.crt' --cert='/etc/kubernetes/pki/etcd/peer.crt' --key='/etc/kubernetes/pki/etcd/peer.key' snapshot save etcd-snapshot.db\n\taws s3 cp etcd-snapshot.db s3://${s3bucket}/etcd-backups/$INSTANCE_ID/etcd-snapshot-\\$(date -Iseconds).db\n\tEOF\n\n  echo \"${backupcron} root bash /usr/local/bin/backup-etcd.sh\" \u003e /etc/cron.d/backup-etcd\n\n  # Poll the spot instance termination URL and backup immediately if it returns a 200 response.\n  cat \u003c\u003c-'EOF' \u003e /usr/local/bin/check-termination.sh\n\t#!/bin/bash\n\t# Mostly borrowed from https://github.com/kube-aws/kube-spot-termination-notice-handler/blob/master/entrypoint.sh\n\t\n\tPOLL_INTERVAL=10\n\tNOTICE_URL=\"http://169.254.169.254/latest/meta-data/spot/termination-time\"\n\t\n\techo \"Polling $${NOTICE_URL} every $${POLL_INTERVAL} second(s)\"\n\t\n\t# To whom it may concern: http://superuser.com/questions/590099/can-i-make-curl-fail-with-an-exitcode-different-than-0-if-the-http-status-code-i\n\twhile http_status=$(curl -o /dev/null -w '%%{http_code}' -sL $${NOTICE_URL}); [ $${http_status} -ne 200 ]; do\n\t  echo \"Polled termination notice URL. HTTP Status was $${http_status}.\"\n\t  sleep $${POLL_INTERVAL}\n\tdone\n\t\n\techo \"Polled termination notice URL. HTTP Status was $${http_status}. Triggering backup.\"\n\t/bin/bash /usr/local/bin/backup-etcd.sh\n\tsleep 300 # Sleep for 5 minutes, by which time the machine will have terminated.\n\tEOF\n\n  cat \u003c\u003c-'EOF' \u003e /etc/systemd/system/check-termination.service\n\t[Unit]\n\tDescription=Spot Termination Checker\n\tAfter=network.target\n\t[Service]\n\tType=simple\n\tRestart=always\n\tRestartSec=10\n\tUser=root\n\tExecStart=/bin/bash /usr/local/bin/check-termination.sh\n\t\n\t[Install]\n\tWantedBy=multi-user.target\n\tEOF\n\n  systemctl start check-termination\n  systemctl enable check-termination\nfi\n\nsu -c \"kubectl get nodes -o wide | grep $${HOSTNAME}\" ubuntu\n",
            "vars": {
              "backupcron": "*/15 * * * *",
              "backupenabled": "1",
              "certmanagerenabled": "1",
              "clustername": "k8s",
              "helmversion": "1.14.3",
              "k8stoken": "xrr5lk.v8fihtfb83fqlsir",
              "k8sversion": "1.15.0",
              "s3bucket": "k8s20190804144427827600000001"
            }
          },
          "depends_on": [
            "aws_s3_bucket.s3-bucket"
          ]
        }
      ]
    },
    {
      "mode": "data",
      "type": "template_file",
      "name": "nginx-ingress-nodeport-manifest",
      "each": "list",
      "provider": "provider.template",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "filename": null,
            "id": "dd8faa715dbcf4480f6a2fdf1b03cca96c2bb32e492c4d033daf7f425c7cdf63",
            "rendered": "apiVersion: v1\nkind: Service\nmetadata:\n  name: ingress-nginx\n  namespace: ingress-nginx\n  labels:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: aws.k8s-russian.video\nspec:\n  type: NodePort\n  ports:\n    - name: http\n      port: 80\n      targetPort: 80\n      protocol: TCP\n    - name: https\n      port: 443\n      targetPort: 443\n      protocol: TCP\n  selector:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n",
            "template": "apiVersion: v1\nkind: Service\nmetadata:\n  name: ingress-nginx\n  namespace: ingress-nginx\n  labels:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: ${nginx_ingress_domain}\nspec:\n  type: NodePort\n  ports:\n    - name: http\n      port: 80\n      targetPort: 80\n      protocol: TCP\n    - name: https\n      port: 443\n      targetPort: 443\n      protocol: TCP\n  selector:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n",
            "vars": {
              "nginx_ingress_domain": "aws.k8s-russian.video"
            }
          }
        }
      ]
    },
    {
      "mode": "data",
      "type": "template_file",
      "name": "worker-userdata",
      "provider": "provider.template",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "filename": null,
            "id": "a4e9043042cba8a8cfd6f62ea2dd01ce8bf162cb1b74dd6d4ff7e99451b0eff4",
            "rendered": "#!/bin/bash -ve\n# kube-router network settings\napt-get install -y ipvsadm\nmodprobe ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh\nmodprobe overlay br_netfilter\n# Setup required sysctl params, these persist across reboots.\ncat \u003e /etc/sysctl.d/99-kubernetes.conf \u003c\u003cEOF\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\nsysctl --system\nservice procps start\n\n# Disable pointless daemons\nsystemctl stop snapd snapd.socket lxcfs snap.amazon-ssm-agent.amazon-ssm-agent\nsystemctl disable snapd snapd.socket lxcfs snap.amazon-ssm-agent.amazon-ssm-agent\n\n# Disable swap to make K8S happy\nswapoff -a\nsed -i '/swap/d' /etc/fstab\n\n# Install K8S, kubeadm and containerd\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\necho \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" \u003e /etc/apt/sources.list.d/kubernetes.list\nexport DEBIAN_FRONTEND=noninteractive\napt-get update\napt-get install -y kubelet=1.15.0-00 kubeadm=1.15.0-00 kubectl=1.15.0-00 containerd.io\napt-mark hold kubelet kubeadm kubectl containerd.io\n\nsystemctl enable containerd\nsystemctl restart containerd\n\n# Point kubelet at big ephemeral drive\nmkdir /mnt/kubelet\necho 'KUBELET_EXTRA_ARGS=\"--root-dir=/mnt/kubelet --cloud-provider=aws\"' \u003e /etc/default/kubelet\n\n# Join the cluster\nfor i in {1..50}; do kubeadm join --token=xrr5lk.v8fihtfb83fqlsir --discovery-token-unsafe-skip-ca-verification --node-name=$(hostname -f) 10.0.100.4:6443 \u0026\u0026 break || sleep 15; done\n",
            "template": "#!/bin/bash -ve\n# kube-router network settings\napt-get install -y ipvsadm\nmodprobe ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh\nmodprobe overlay br_netfilter\n# Setup required sysctl params, these persist across reboots.\ncat \u003e /etc/sysctl.d/99-kubernetes.conf \u003c\u003cEOF\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\nsysctl --system\nservice procps start\n\n# Disable pointless daemons\nsystemctl stop snapd snapd.socket lxcfs snap.amazon-ssm-agent.amazon-ssm-agent\nsystemctl disable snapd snapd.socket lxcfs snap.amazon-ssm-agent.amazon-ssm-agent\n\n# Disable swap to make K8S happy\nswapoff -a\nsed -i '/swap/d' /etc/fstab\n\n# Install K8S, kubeadm and containerd\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\necho \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" \u003e /etc/apt/sources.list.d/kubernetes.list\nexport DEBIAN_FRONTEND=noninteractive\napt-get update\napt-get install -y kubelet=${k8sversion}-00 kubeadm=${k8sversion}-00 kubectl=${k8sversion}-00 containerd.io\napt-mark hold kubelet kubeadm kubectl containerd.io\n\nsystemctl enable containerd\nsystemctl restart containerd\n\n# Point kubelet at big ephemeral drive\nmkdir /mnt/kubelet\necho 'KUBELET_EXTRA_ARGS=\"--root-dir=/mnt/kubelet --cloud-provider=aws\"' \u003e /etc/default/kubelet\n\n# Join the cluster\nfor i in {1..50}; do kubeadm join --token=${k8stoken} --discovery-token-unsafe-skip-ca-verification --node-name=$(hostname -f) ${masterIP}:6443 \u0026\u0026 break || sleep 15; done\n",
            "vars": {
              "k8stoken": "xrr5lk.v8fihtfb83fqlsir",
              "k8sversion": "1.15.0",
              "masterIP": "10.0.100.4"
            }
          }
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_autoscaling_group",
      "name": "worker",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:autoscaling:us-east-1:324305410971:autoScalingGroup:2aa2c92b-3b37-4697-a4fd-bb5bd5e86b7a:autoScalingGroupName/k8s-worker",
            "availability_zones": [
              "us-east-1a"
            ],
            "default_cooldown": 300,
            "desired_capacity": 1,
            "enabled_metrics": null,
            "force_delete": false,
            "health_check_grace_period": 300,
            "health_check_type": "EC2",
            "id": "k8s-worker",
            "initial_lifecycle_hook": [],
            "launch_configuration": "",
            "launch_template": [
              {
                "id": "lt-09727b7013ab63797",
                "name": "k8s-worker",
                "version": "$Latest"
              }
            ],
            "load_balancers": [],
            "max_size": 1,
            "metrics_granularity": "1Minute",
            "min_elb_capacity": null,
            "min_size": 1,
            "mixed_instances_policy": [],
            "name": "k8s-worker",
            "name_prefix": null,
            "placement_group": "",
            "protect_from_scale_in": false,
            "service_linked_role_arn": "arn:aws:iam::324305410971:role/aws-service-role/autoscaling.amazonaws.com/AWSServiceRoleForAutoScaling",
            "suspended_processes": null,
            "tag": [
              {
                "key": "Environment",
                "propagate_at_launch": true,
                "value": "k8s"
              },
              {
                "key": "Name",
                "propagate_at_launch": true,
                "value": "k8s-worker"
              },
              {
                "key": "k8s.io/cluster-autoscaler/enabled",
                "propagate_at_launch": true,
                "value": "true"
              },
              {
                "key": "kubernetes.io/cluster/k8s",
                "propagate_at_launch": true,
                "value": "owned"
              }
            ],
            "tags": null,
            "target_group_arns": [],
            "termination_policies": null,
            "timeouts": null,
            "vpc_zone_identifier": [
              "subnet-051984d12989c23c7"
            ],
            "wait_for_capacity_timeout": "10m",
            "wait_for_elb_capacity": null
          },
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjo2MDAwMDAwMDAwMDB9fQ==",
          "depends_on": [
            "aws_launch_template.worker",
            "aws_subnet.public"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_instance_profile",
      "name": "profile",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::324305410971:instance-profile/k8s-instance-profile",
            "create_date": "2019-08-04T16:00:46Z",
            "id": "k8s-instance-profile",
            "name": "k8s-instance-profile",
            "name_prefix": null,
            "path": "/",
            "role": "k8s-instance-role",
            "roles": [
              "k8s-instance-role"
            ],
            "unique_id": "AIPAUXAQ33ONWW26G5PTZ"
          },
          "private": "bnVsbA==",
          "depends_on": [
            "aws_iam_role.role"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_policy",
      "name": "autoscaling",
      "each": "list",
      "provider": "provider.aws",
      "instances": []
    },
    {
      "mode": "managed",
      "type": "aws_iam_policy",
      "name": "cluster-policy",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::324305410971:policy/k8s-cluster-policy",
            "description": "Policy for k8s cluster to allow dynamic provisioning of EBS persistent volumes",
            "id": "arn:aws:iam::324305410971:policy/k8s-cluster-policy",
            "name": "k8s-cluster-policy",
            "name_prefix": null,
            "path": "/",
            "policy": "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:AttachVolume\",\n                \"ec2:CreateVolume\",\n                \"ec2:DeleteVolume\",\n                \"ec2:DescribeInstances\",\n                \"ec2:DescribeRouteTables\",\n                \"ec2:DescribeSecurityGroups\",\n                \"ec2:DescribeSubnets\",\n                \"ec2:DescribeVolumes\",\n                \"ec2:DescribeVolumesModifications\",\n                \"ec2:DescribeVpcs\",\n                \"elasticloadbalancing:DescribeLoadBalancers\",\n                \"ec2:DetachVolume\",\n                \"ec2:ModifyVolume\",\n                \"ec2:CreateTags\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n"
          },
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_policy",
      "name": "route53-policy",
      "each": "list",
      "provider": "provider.aws",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::324305410971:policy/k8s-route53-policy",
            "description": "Polcy for k8s cluster to allow access to Route 53 for DNS record creation",
            "id": "arn:aws:iam::324305410971:policy/k8s-route53-policy",
            "name": "k8s-route53-policy",
            "name_prefix": null,
            "path": "/",
            "policy": "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\"route53:*\"],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n"
          },
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_policy",
      "name": "s3-bucket-policy",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::324305410971:policy/k8s-s3-bucket-policy",
            "description": "Polcy for k8s cluster to allow access to the Backup S3 Bucket",
            "id": "arn:aws:iam::324305410971:policy/k8s-s3-bucket-policy",
            "name": "k8s-s3-bucket-policy",
            "name_prefix": null,
            "path": "/",
            "policy": "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\"s3:ListBucket\"],\n            \"Resource\": [\"arn:aws:s3:::k8s20190804144427827600000001\"]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:PutObject\",\n                \"s3:GetObject\",\n                \"s3:ListObjects\"\n            ],\n            \"Resource\": [\"arn:aws:s3:::k8s20190804144427827600000001/*\"]\n        }\n    ]\n}\n"
          },
          "private": "bnVsbA==",
          "depends_on": [
            "aws_s3_bucket.s3-bucket"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role",
      "name": "role",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::324305410971:role/k8s-instance-role",
            "assume_role_policy": "{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"ec2.amazonaws.com\"},\"Action\":\"sts:AssumeRole\"}]}",
            "create_date": "2019-08-04T15:03:45Z",
            "description": "",
            "force_detach_policies": false,
            "id": "k8s-instance-role",
            "max_session_duration": 3600,
            "name": "k8s-instance-role",
            "name_prefix": null,
            "path": "/",
            "permissions_boundary": null,
            "tags": {},
            "unique_id": "AROAUXAQ33ONSVFTIKUCZ"
          },
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "autoscaling",
      "each": "list",
      "provider": "provider.aws",
      "instances": []
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "cluster-policy",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "k8s-instance-role-20190804161120686700000002",
            "policy_arn": "arn:aws:iam::324305410971:policy/k8s-cluster-policy",
            "role": "k8s-instance-role"
          },
          "private": "bnVsbA==",
          "depends_on": [
            "aws_iam_policy.cluster-policy",
            "aws_iam_role.role"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "policy",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "k8s-instance-role-20190804150347180400000001",
            "policy_arn": "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly",
            "role": "k8s-instance-role"
          },
          "private": "bnVsbA==",
          "depends_on": [
            "aws_iam_role.role"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "route53-policy",
      "each": "list",
      "provider": "provider.aws",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "id": "k8s-instance-role-20190804161120686700000003",
            "policy_arn": "arn:aws:iam::324305410971:policy/k8s-route53-policy",
            "role": "k8s-instance-role"
          },
          "private": "bnVsbA==",
          "depends_on": [
            "aws_iam_policy.route53-policy",
            "aws_iam_role.role"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "s3-bucket-policy",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "k8s-instance-role-20190804161120552100000001",
            "policy_arn": "arn:aws:iam::324305410971:policy/k8s-s3-bucket-policy",
            "role": "k8s-instance-role"
          },
          "private": "bnVsbA==",
          "depends_on": [
            "aws_iam_policy.s3-bucket-policy",
            "aws_iam_role.role"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_internet_gateway",
      "name": "gw",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "igw-09fe63a4c920fbc11",
            "owner_id": "324305410971",
            "tags": {
              "Environment": "k8s",
              "Name": "k8s"
            },
            "vpc_id": "vpc-0cea81c6c989df0a9"
          },
          "private": "bnVsbA==",
          "depends_on": [
            "aws_vpc.main"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_launch_template",
      "name": "worker",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:ec2:us-east-1:324305410971:launch-template/lt-09727b7013ab63797",
            "block_device_mappings": [],
            "capacity_reservation_specification": [],
            "credit_specification": [],
            "default_version": 1,
            "description": "",
            "disable_api_termination": false,
            "ebs_optimized": "false",
            "elastic_gpu_specifications": [],
            "elastic_inference_accelerator": [],
            "iam_instance_profile": [
              {
                "arn": "",
                "name": "k8s-instance-profile"
              }
            ],
            "id": "lt-09727b7013ab63797",
            "image_id": "ami-07d0cf3af28718ef8",
            "instance_initiated_shutdown_behavior": "",
            "instance_market_options": [
              {
                "market_type": "spot",
                "spot_options": [
                  {
                    "block_duration_minutes": 0,
                    "instance_interruption_behavior": "",
                    "max_price": "0.01",
                    "spot_instance_type": "",
                    "valid_until": ""
                  }
                ]
              }
            ],
            "instance_type": "t3.small",
            "kernel_id": "",
            "key_name": "kubeadm-aws",
            "latest_version": 3,
            "license_specification": [],
            "monitoring": [],
            "name": "k8s-worker",
            "name_prefix": null,
            "network_interfaces": [],
            "placement": [],
            "ram_disk_id": "",
            "security_group_names": [],
            "tag_specifications": [],
            "tags": {},
            "user_data": "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",
            "vpc_security_group_ids": [
              "sg-003f1486df21a986b"
            ]
          },
          "private": "bnVsbA==",
          "depends_on": [
            "aws_iam_instance_profile.profile",
            "aws_security_group.kubernetes",
            "data.aws_ami.latest_ami",
            "data.template_file.worker-userdata"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_route_table",
      "name": "r",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "rtb-03e512fc8246fa2cd",
            "owner_id": "324305410971",
            "propagating_vgws": [],
            "route": [
              {
                "cidr_block": "0.0.0.0/0",
                "egress_only_gateway_id": "",
                "gateway_id": "igw-09fe63a4c920fbc11",
                "instance_id": "",
                "ipv6_cidr_block": "",
                "nat_gateway_id": "",
                "network_interface_id": "",
                "transit_gateway_id": "",
                "vpc_peering_connection_id": ""
              }
            ],
            "tags": {
              "Environment": "k8s",
              "Name": "k8s"
            },
            "vpc_id": "vpc-0cea81c6c989df0a9"
          },
          "private": "bnVsbA==",
          "depends_on": [
            "aws_internet_gateway.gw",
            "aws_vpc.main"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_route_table_association",
      "name": "public",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "rtbassoc-0fe1dcda571aa967b",
            "route_table_id": "rtb-03e512fc8246fa2cd",
            "subnet_id": "subnet-051984d12989c23c7"
          },
          "private": "bnVsbA==",
          "depends_on": [
            "aws_route_table.r",
            "aws_subnet.public"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_s3_bucket",
      "name": "s3-bucket",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "acceleration_status": "",
            "acl": "private",
            "arn": "arn:aws:s3:::k8s20190804144427827600000001",
            "bucket": "k8s20190804144427827600000001",
            "bucket_domain_name": "k8s20190804144427827600000001.s3.amazonaws.com",
            "bucket_prefix": "k8s",
            "bucket_regional_domain_name": "k8s20190804144427827600000001.s3.amazonaws.com",
            "cors_rule": [],
            "force_destroy": true,
            "hosted_zone_id": "Z3AQBSTGFYJSTF",
            "id": "k8s20190804144427827600000001",
            "lifecycle_rule": [
              {
                "abort_incomplete_multipart_upload_days": 0,
                "enabled": true,
                "expiration": [
                  {
                    "date": "",
                    "days": 7,
                    "expired_object_delete_marker": false
                  }
                ],
                "id": "etcd-backups",
                "noncurrent_version_expiration": [],
                "noncurrent_version_transition": [],
                "prefix": "etcd-backups/",
                "tags": {},
                "transition": []
              }
            ],
            "logging": [],
            "object_lock_configuration": [],
            "policy": null,
            "region": "us-east-1",
            "replication_configuration": [],
            "request_payer": "BucketOwner",
            "server_side_encryption_configuration": [],
            "tags": {
              "Environment": "k8s"
            },
            "versioning": [
              {
                "enabled": false,
                "mfa_delete": false
              }
            ],
            "website": [],
            "website_domain": null,
            "website_endpoint": null
          },
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_s3_bucket_object",
      "name": "cert-manager-issuer-manifest",
      "each": "list",
      "provider": "provider.aws",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "acl": "private",
            "bucket": "k8s20190804144427827600000001",
            "cache_control": "",
            "content": "apiVersion: certmanager.k8s.io/v1beta1\nkind: Issuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    # The ACME server URL\n    server: https://acme-v02.api.letsencrypt.org/directory\n    # Email address used for ACME registration\n    email: bloodjazman@gmail.com\n    # Name of a secret used to store the ACME account private key\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    # Enable the HTTP-01 challenge provider\n    http01: {}\n",
            "content_base64": null,
            "content_disposition": "",
            "content_encoding": "",
            "content_language": "",
            "content_type": "binary/octet-stream",
            "etag": "513dde98a6116e9159f895ed4a384c18",
            "id": "manifests/cert-manager-issuer.yaml",
            "key": "manifests/cert-manager-issuer.yaml",
            "kms_key_id": null,
            "server_side_encryption": "",
            "source": null,
            "storage_class": "STANDARD",
            "tags": {},
            "version_id": "",
            "website_redirect": ""
          },
          "private": "bnVsbA==",
          "depends_on": [
            "aws_s3_bucket.s3-bucket",
            "data.template_file.cert-manager-issuer-manifest"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_s3_bucket_object",
      "name": "cluster-autoscaler-manifest",
      "each": "list",
      "provider": "provider.aws",
      "instances": []
    },
    {
      "mode": "managed",
      "type": "aws_s3_bucket_object",
      "name": "ebs-storage-class-manifest",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "acl": "private",
            "bucket": "k8s20190804144427827600000001",
            "cache_control": "",
            "content": null,
            "content_base64": null,
            "content_disposition": "",
            "content_encoding": "",
            "content_language": "",
            "content_type": "binary/octet-stream",
            "etag": "29b34fca66311fbc22d25ed88b526c9d",
            "id": "manifests/ebs-storage-class.yaml",
            "key": "manifests/ebs-storage-class.yaml",
            "kms_key_id": null,
            "server_side_encryption": "",
            "source": "manifests/ebs-storage-class.yaml",
            "storage_class": "STANDARD",
            "tags": {},
            "version_id": "",
            "website_redirect": ""
          },
          "private": "bnVsbA==",
          "depends_on": [
            "aws_s3_bucket.s3-bucket"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_s3_bucket_object",
      "name": "external-dns-manifest",
      "each": "list",
      "provider": "provider.aws",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "acl": "private",
            "bucket": "k8s20190804144427827600000001",
            "cache_control": "",
            "content": null,
            "content_base64": null,
            "content_disposition": "",
            "content_encoding": "",
            "content_language": "",
            "content_type": "binary/octet-stream",
            "etag": "2df698628b51ca0c25b0ed7feb5f3922",
            "id": "manifests/external-dns.yaml",
            "key": "manifests/external-dns.yaml",
            "kms_key_id": null,
            "server_side_encryption": "",
            "source": "manifests/external-dns.yaml",
            "storage_class": "STANDARD",
            "tags": {},
            "version_id": "",
            "website_redirect": ""
          },
          "private": "bnVsbA==",
          "depends_on": [
            "aws_s3_bucket.s3-bucket"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_s3_bucket_object",
      "name": "nginx-ingress-manifest",
      "each": "list",
      "provider": "provider.aws",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "acl": "private",
            "bucket": "k8s20190804144427827600000001",
            "cache_control": "",
            "content": null,
            "content_base64": null,
            "content_disposition": "",
            "content_encoding": "",
            "content_language": "",
            "content_type": "binary/octet-stream",
            "etag": "b19245ae8248a25e7423f1c1b0f179df",
            "id": "manifests/nginx-ingress-mandatory.yaml",
            "key": "manifests/nginx-ingress-mandatory.yaml",
            "kms_key_id": null,
            "server_side_encryption": "",
            "source": "manifests/nginx-ingress-mandatory.yaml",
            "storage_class": "STANDARD",
            "tags": {},
            "version_id": "",
            "website_redirect": ""
          },
          "private": "bnVsbA==",
          "depends_on": [
            "aws_s3_bucket.s3-bucket"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_s3_bucket_object",
      "name": "nginx-ingress-nodeport-manifest",
      "each": "list",
      "provider": "provider.aws",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "acl": "private",
            "bucket": "k8s20190804144427827600000001",
            "cache_control": "",
            "content": "apiVersion: v1\nkind: Service\nmetadata:\n  name: ingress-nginx\n  namespace: ingress-nginx\n  labels:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: aws.k8s-russian.video\nspec:\n  type: NodePort\n  ports:\n    - name: http\n      port: 80\n      targetPort: 80\n      protocol: TCP\n    - name: https\n      port: 443\n      targetPort: 443\n      protocol: TCP\n  selector:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n",
            "content_base64": null,
            "content_disposition": "",
            "content_encoding": "",
            "content_language": "",
            "content_type": "binary/octet-stream",
            "etag": "78d81fac0d3d4693e3699983390a5735",
            "id": "manifests/nginx-ingress-nodeport.yaml",
            "key": "manifests/nginx-ingress-nodeport.yaml",
            "kms_key_id": null,
            "server_side_encryption": "",
            "source": null,
            "storage_class": "STANDARD",
            "tags": {},
            "version_id": "",
            "website_redirect": ""
          },
          "private": "bnVsbA==",
          "depends_on": [
            "aws_s3_bucket.s3-bucket",
            "data.template_file.nginx-ingress-nodeport-manifest"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_security_group",
      "name": "kubernetes",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:us-east-1:324305410971:security-group/sg-003f1486df21a986b",
            "description": "Allow inbound ssh traffic",
            "egress": [
              {
                "cidr_blocks": [
                  "0.0.0.0/0"
                ],
                "description": "",
                "from_port": 0,
                "ipv6_cidr_blocks": [],
                "prefix_list_ids": [],
                "protocol": "-1",
                "security_groups": [],
                "self": false,
                "to_port": 0
              }
            ],
            "id": "sg-003f1486df21a986b",
            "ingress": [
              {
                "cidr_blocks": [
                  "0.0.0.0/0"
                ],
                "description": "",
                "from_port": 22,
                "ipv6_cidr_blocks": [],
                "prefix_list_ids": [],
                "protocol": "tcp",
                "security_groups": [],
                "self": false,
                "to_port": 22
              },
              {
                "cidr_blocks": [
                  "0.0.0.0/0"
                ],
                "description": "",
                "from_port": 443,
                "ipv6_cidr_blocks": [],
                "prefix_list_ids": [],
                "protocol": "tcp",
                "security_groups": [],
                "self": false,
                "to_port": 443
              },
              {
                "cidr_blocks": [
                  "0.0.0.0/0"
                ],
                "description": "",
                "from_port": 6443,
                "ipv6_cidr_blocks": [],
                "prefix_list_ids": [],
                "protocol": "tcp",
                "security_groups": [],
                "self": false,
                "to_port": 6443
              },
              {
                "cidr_blocks": [
                  "0.0.0.0/0"
                ],
                "description": "",
                "from_port": 80,
                "ipv6_cidr_blocks": [],
                "prefix_list_ids": [],
                "protocol": "tcp",
                "security_groups": [],
                "self": false,
                "to_port": 80
              },
              {
                "cidr_blocks": [],
                "description": "",
                "from_port": 0,
                "ipv6_cidr_blocks": [],
                "prefix_list_ids": [],
                "protocol": "-1",
                "security_groups": [],
                "self": true,
                "to_port": 0
              }
            ],
            "name": "k8s",
            "name_prefix": null,
            "owner_id": "324305410971",
            "revoke_rules_on_delete": false,
            "tags": {
              "Environment": "k8s",
              "Name": "k8s"
            },
            "timeouts": null,
            "vpc_id": "vpc-0cea81c6c989df0a9"
          },
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6NjAwMDAwMDAwMDAwfSwic2NoZW1hX3ZlcnNpb24iOiIxIn0=",
          "depends_on": [
            "aws_vpc.main"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_security_group_rule",
      "name": "allow_all_from_self",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 2,
          "attributes": {
            "cidr_blocks": [],
            "description": "",
            "from_port": 0,
            "id": "sgrule-1595710673",
            "ipv6_cidr_blocks": [],
            "prefix_list_ids": [],
            "protocol": "-1",
            "security_group_id": "sg-003f1486df21a986b",
            "self": false,
            "source_security_group_id": "sg-003f1486df21a986b",
            "to_port": 0,
            "type": "ingress"
          },
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjIifQ==",
          "depends_on": [
            "aws_security_group.kubernetes"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_security_group_rule",
      "name": "allow_all_out",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 2,
          "attributes": {
            "cidr_blocks": [
              "0.0.0.0/0"
            ],
            "description": "",
            "from_port": 0,
            "id": "sgrule-2995003657",
            "ipv6_cidr_blocks": [],
            "prefix_list_ids": [],
            "protocol": "-1",
            "security_group_id": "sg-003f1486df21a986b",
            "self": false,
            "source_security_group_id": null,
            "to_port": 0,
            "type": "egress"
          },
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjIifQ==",
          "depends_on": [
            "aws_security_group.kubernetes"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_security_group_rule",
      "name": "allow_http_from_web",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 2,
          "attributes": {
            "cidr_blocks": [
              "0.0.0.0/0"
            ],
            "description": "",
            "from_port": 80,
            "id": "sgrule-1650483330",
            "ipv6_cidr_blocks": [],
            "prefix_list_ids": [],
            "protocol": "tcp",
            "security_group_id": "sg-003f1486df21a986b",
            "self": false,
            "source_security_group_id": null,
            "to_port": 80,
            "type": "ingress"
          },
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjIifQ==",
          "depends_on": [
            "aws_security_group.kubernetes"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_security_group_rule",
      "name": "allow_https_from_web",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 2,
          "attributes": {
            "cidr_blocks": [
              "0.0.0.0/0"
            ],
            "description": "",
            "from_port": 443,
            "id": "sgrule-3794049795",
            "ipv6_cidr_blocks": [],
            "prefix_list_ids": [],
            "protocol": "tcp",
            "security_group_id": "sg-003f1486df21a986b",
            "self": false,
            "source_security_group_id": null,
            "to_port": 443,
            "type": "ingress"
          },
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjIifQ==",
          "depends_on": [
            "aws_security_group.kubernetes"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_security_group_rule",
      "name": "allow_k8s_from_admin",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 2,
          "attributes": {
            "cidr_blocks": [
              "0.0.0.0/0"
            ],
            "description": "",
            "from_port": 6443,
            "id": "sgrule-975545922",
            "ipv6_cidr_blocks": [],
            "prefix_list_ids": [],
            "protocol": "tcp",
            "security_group_id": "sg-003f1486df21a986b",
            "self": false,
            "source_security_group_id": null,
            "to_port": 6443,
            "type": "ingress"
          },
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjIifQ==",
          "depends_on": [
            "aws_security_group.kubernetes"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_security_group_rule",
      "name": "allow_ssh_from_admin",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 2,
          "attributes": {
            "cidr_blocks": [
              "0.0.0.0/0"
            ],
            "description": "",
            "from_port": 22,
            "id": "sgrule-3572120887",
            "ipv6_cidr_blocks": [],
            "prefix_list_ids": [],
            "protocol": "tcp",
            "security_group_id": "sg-003f1486df21a986b",
            "self": false,
            "source_security_group_id": null,
            "to_port": 22,
            "type": "ingress"
          },
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjIifQ==",
          "depends_on": [
            "aws_security_group.kubernetes"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_spot_instance_request",
      "name": "master",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "ami": "ami-07d0cf3af28718ef8",
            "arn": null,
            "associate_public_ip_address": true,
            "availability_zone": null,
            "block_duration_minutes": 0,
            "cpu_core_count": null,
            "cpu_threads_per_core": null,
            "credit_specification": [],
            "disable_api_termination": null,
            "ebs_block_device": [],
            "ebs_optimized": null,
            "ephemeral_block_device": [],
            "get_password_data": false,
            "host_id": null,
            "iam_instance_profile": "k8s-instance-profile",
            "id": "sir-wen87jvh",
            "instance_initiated_shutdown_behavior": null,
            "instance_interruption_behaviour": "terminate",
            "instance_state": null,
            "instance_type": "t3.small",
            "ipv6_address_count": 0,
            "ipv6_addresses": [],
            "key_name": "kubeadm-aws",
            "launch_group": "",
            "monitoring": null,
            "network_interface": [],
            "network_interface_id": "eni-0c6f7ab8c149203fa",
            "password_data": "",
            "placement_group": null,
            "primary_network_interface_id": null,
            "private_dns": "ip-10-0-100-4.ec2.internal",
            "private_ip": "10.0.100.4",
            "public_dns": "ec2-52-90-121-0.compute-1.amazonaws.com",
            "public_ip": "52.90.121.0",
            "root_block_device": [
              {
                "delete_on_termination": true,
                "iops": 100,
                "volume_id": "vol-0d09bfb754d6ee619",
                "volume_size": 8,
                "volume_type": "gp2"
              }
            ],
            "security_groups": [],
            "source_dest_check": true,
            "spot_bid_status": "fulfilled",
            "spot_instance_id": "i-0a624eddc797cf04f",
            "spot_price": "0.01",
            "spot_request_state": "active",
            "spot_type": "persistent",
            "subnet_id": "subnet-051984d12989c23c7",
            "tags": {
              "Environment": "k8s",
              "Name": "k8s-master"
            },
            "tenancy": null,
            "timeouts": null,
            "user_data": "4963b93de5932f2d616a4fa395318fe91657dae4",
            "user_data_base64": null,
            "valid_from": "0001-01-01T00:00:00Z",
            "valid_until": "9999-12-25T12:00:00Z",
            "volume_tags": null,
            "vpc_security_group_ids": [
              "sg-003f1486df21a986b"
            ],
            "wait_for_fulfillment": true
          },
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6MTIwMDAwMDAwMDAwMH19",
          "depends_on": [
            "aws_iam_instance_profile.profile",
            "aws_internet_gateway.gw",
            "aws_security_group.kubernetes",
            "aws_subnet.public",
            "data.aws_ami.latest_ami",
            "data.template_file.master-userdata"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_subnet",
      "name": "public",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:us-east-1:324305410971:subnet/subnet-051984d12989c23c7",
            "assign_ipv6_address_on_creation": false,
            "availability_zone": "us-east-1a",
            "availability_zone_id": "use1-az6",
            "cidr_block": "10.0.100.0/24",
            "id": "subnet-051984d12989c23c7",
            "ipv6_cidr_block": "",
            "ipv6_cidr_block_association_id": "",
            "map_public_ip_on_launch": true,
            "owner_id": "324305410971",
            "tags": {
              "Environment": "k8s",
              "Name": "k8s"
            },
            "timeouts": null,
            "vpc_id": "vpc-0cea81c6c989df0a9"
          },
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6MTIwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiMSJ9",
          "depends_on": [
            "aws_vpc.main"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_vpc",
      "name": "main",
      "provider": "provider.aws",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:us-east-1:324305410971:vpc/vpc-0cea81c6c989df0a9",
            "assign_generated_ipv6_cidr_block": false,
            "cidr_block": "10.0.0.0/16",
            "default_network_acl_id": "acl-00720ca1600db9766",
            "default_route_table_id": "rtb-04b7da4e8b895fa40",
            "default_security_group_id": "sg-0282615f6af1a96e9",
            "dhcp_options_id": "dopt-6e615a15",
            "enable_classiclink": false,
            "enable_classiclink_dns_support": false,
            "enable_dns_hostnames": true,
            "enable_dns_support": true,
            "id": "vpc-0cea81c6c989df0a9",
            "instance_tenancy": "default",
            "ipv6_association_id": "",
            "ipv6_cidr_block": "",
            "main_route_table_id": "rtb-04b7da4e8b895fa40",
            "owner_id": "324305410971",
            "tags": {
              "Environment": "k8s",
              "Name": "k8s"
            }
          },
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "random_string",
      "name": "k8stoken-first-part",
      "provider": "provider.random",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "id": "none",
            "keepers": null,
            "length": 6,
            "lower": true,
            "min_lower": 0,
            "min_numeric": 0,
            "min_special": 0,
            "min_upper": 0,
            "number": true,
            "override_special": null,
            "result": "xrr5lk",
            "special": false,
            "upper": false
          }
        }
      ]
    },
    {
      "mode": "managed",
      "type": "random_string",
      "name": "k8stoken-second-part",
      "provider": "provider.random",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "id": "none",
            "keepers": null,
            "length": 16,
            "lower": true,
            "min_lower": 0,
            "min_numeric": 0,
            "min_special": 0,
            "min_upper": 0,
            "number": true,
            "override_special": null,
            "result": "v8fihtfb83fqlsir",
            "special": false,
            "upper": false
          }
        }
      ]
    }
  ]
}
